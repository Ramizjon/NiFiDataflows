<?xml version="1.0" encoding="UTF-8" standalone="yes"?><template><description>Reads data from kafka topic to HDFS </description><name>KafkaToHDFSDataflow</name><snippet><connections><id>653203dd-435d-445c-8295-50984a32da7a</id><parentGroupId>cc088de2-ba5b-425b-a987-13d1694e46ed</parentGroupId><backPressureDataSizeThreshold>0 MB</backPressureDataSizeThreshold><backPressureObjectThreshold>0</backPressureObjectThreshold><destination><groupId>cc088de2-ba5b-425b-a987-13d1694e46ed</groupId><id>0c05eaf6-6c5a-4aec-a22d-6547a6e3bc5b</id><type>PROCESSOR</type></destination><flowFileExpiration>0 sec</flowFileExpiration><labelIndex>1</labelIndex><name>KafkaToHDFS</name><selectedRelationships>success</selectedRelationships><source><groupId>cc088de2-ba5b-425b-a987-13d1694e46ed</groupId><id>a139a4d0-5365-434e-82d3-0e306c2c9ac4</id><type>PROCESSOR</type></source><zIndex>0</zIndex></connections><processors><id>a139a4d0-5365-434e-82d3-0e306c2c9ac4</id><parentGroupId>cc088de2-ba5b-425b-a987-13d1694e46ed</parentGroupId><position><x>1454.0142916044572</x><y>998.7777194199182</y></position><config><bulletinLevel>WARN</bulletinLevel><comments></comments><concurrentlySchedulableTaskCount>2</concurrentlySchedulableTaskCount><defaultConcurrentTasks><entry><key>TIMER_DRIVEN</key><value>1</value></entry><entry><key>EVENT_DRIVEN</key><value>0</value></entry><entry><key>CRON_DRIVEN</key><value>1</value></entry></defaultConcurrentTasks><defaultSchedulingPeriod><entry><key>TIMER_DRIVEN</key><value>0 sec</value></entry><entry><key>CRON_DRIVEN</key><value>* * * * * ?</value></entry></defaultSchedulingPeriod><descriptors><entry><key>ZooKeeper Connection String</key><value><description>The Connection String to use in order to connect to ZooKeeper. This is often a comma-separated list of &lt;host&gt;:&lt;port&gt; combinations. For example, host1:2181,host2:2181,host3:2188</description><displayName>ZooKeeper Connection String</displayName><dynamic>false</dynamic><name>ZooKeeper Connection String</name><required>true</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry><entry><key>Topic Name</key><value><description>The Kafka Topic to pull messages from</description><displayName>Topic Name</displayName><dynamic>false</dynamic><name>Topic Name</name><required>true</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry><entry><key>Zookeeper Commit Frequency</key><value><defaultValue>60 secs</defaultValue><description>Specifies how often to communicate with ZooKeeper to indicate which messages have been pulled. A longer time period will result in better overall performance but can result in more data duplication if a NiFi node is lost</description><displayName>Zookeeper Commit Frequency</displayName><dynamic>false</dynamic><name>Zookeeper Commit Frequency</name><required>true</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry><entry><key>Batch Size</key><value><defaultValue>1</defaultValue><description>Specifies the maximum number of messages to combine into a single FlowFile. These messages will be concatenated together with the &lt;Message Demarcator&gt; string placed between the content of each message. If the messages from Kafka should not be concatenated together, leave this value at 1.</description><displayName>Batch Size</displayName><dynamic>false</dynamic><name>Batch Size</name><required>true</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry><entry><key>Message Demarcator</key><value><defaultValue>\n</defaultValue><description>Specifies the characters to use in order to demarcate multiple messages from Kafka. If the &lt;Batch Size&gt; property is set to 1, this value is ignored. Otherwise, for each two subsequent messages in the batch, this value will be placed in between them.</description><displayName>Message Demarcator</displayName><dynamic>false</dynamic><name>Message Demarcator</name><required>true</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry><entry><key>Client Name</key><value><defaultValue>NiFi-a139a4d0-5365-434e-82d3-0e306c2c9ac4</defaultValue><description>Client Name to use when communicating with Kafka</description><displayName>Client Name</displayName><dynamic>false</dynamic><name>Client Name</name><required>true</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry><entry><key>Group ID</key><value><defaultValue>a139a4d0-5365-434e-82d3-0e306c2c9ac4</defaultValue><description>A Group ID is used to identify consumers that are within the same consumer group</description><displayName>Group ID</displayName><dynamic>false</dynamic><name>Group ID</name><required>true</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry><entry><key>Kafka Communications Timeout</key><value><defaultValue>30 secs</defaultValue><description>The amount of time to wait for a response from Kafka before determining that there is a communications error</description><displayName>Kafka Communications Timeout</displayName><dynamic>false</dynamic><name>Kafka Communications Timeout</name><required>true</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry><entry><key>ZooKeeper Communications Timeout</key><value><defaultValue>30 secs</defaultValue><description>The amount of time to wait for a response from ZooKeeper before determining that there is a communications error</description><displayName>ZooKeeper Communications Timeout</displayName><dynamic>false</dynamic><name>ZooKeeper Communications Timeout</name><required>true</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry><entry><key>Auto Offset Reset</key><value><allowableValues><displayName>smallest</displayName><value>smallest</value></allowableValues><allowableValues><displayName>largest</displayName><value>largest</value></allowableValues><defaultValue>largest</defaultValue><description>Automatically reset the offset to the smallest or largest offset available on the broker</description><displayName>Auto Offset Reset</displayName><dynamic>false</dynamic><name>Auto Offset Reset</name><required>true</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry></descriptors><lossTolerant>false</lossTolerant><penaltyDuration>30 sec</penaltyDuration><properties><entry><key>ZooKeeper Connection String</key><value>localhost:2181</value></entry><entry><key>Topic Name</key><value>facebookprovider</value></entry><entry><key>Zookeeper Commit Frequency</key><value>60 secs</value></entry><entry><key>Batch Size</key><value>6000</value></entry><entry><key>Message Demarcator</key><value>\n</value></entry><entry><key>Client Name</key><value>NiFi-a139a4d0-5365-434e-82d3-0e306c2c9ac4</value></entry><entry><key>Group ID</key><value>a139a4d0-5365-434e-82d3-0e306c2c9ac4</value></entry><entry><key>Kafka Communications Timeout</key><value>30 secs</value></entry><entry><key>ZooKeeper Communications Timeout</key><value>30 secs</value></entry><entry><key>Auto Offset Reset</key><value>largest</value></entry></properties><runDurationMillis>0</runDurationMillis><schedulingPeriod>0 sec</schedulingPeriod><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><yieldDuration>1 sec</yieldDuration></config><name>GetKafka</name><relationships><autoTerminate>false</autoTerminate><description>All FlowFiles that are created are routed to this relationship</description><name>success</name></relationships><state>STOPPED</state><style/><supportsEventDriven>false</supportsEventDriven><supportsParallelProcessing>true</supportsParallelProcessing><type>org.apache.nifi.processors.kafka.GetKafka</type></processors><processors><id>0c05eaf6-6c5a-4aec-a22d-6547a6e3bc5b</id><parentGroupId>cc088de2-ba5b-425b-a987-13d1694e46ed</parentGroupId><position><x>2185.3337381506394</x><y>1004.9573587546383</y></position><config><bulletinLevel>WARN</bulletinLevel><comments></comments><concurrentlySchedulableTaskCount>1</concurrentlySchedulableTaskCount><defaultConcurrentTasks><entry><key>TIMER_DRIVEN</key><value>1</value></entry><entry><key>EVENT_DRIVEN</key><value>0</value></entry><entry><key>CRON_DRIVEN</key><value>1</value></entry></defaultConcurrentTasks><defaultSchedulingPeriod><entry><key>TIMER_DRIVEN</key><value>0 sec</value></entry><entry><key>CRON_DRIVEN</key><value>* * * * * ?</value></entry></defaultSchedulingPeriod><descriptors><entry><key>Hadoop Configuration Resources</key><value><description>A file or comma separated list of files which contains the Hadoop file system configuration. Without this, Hadoop will search the classpath for a 'core-site.xml' and 'hdfs-site.xml' file or will revert to a default configuration.</description><displayName>Hadoop Configuration Resources</displayName><dynamic>false</dynamic><name>Hadoop Configuration Resources</name><required>false</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry><entry><key>Kerberos Principal</key><value><description>Kerberos principal to authenticate as. Requires nifi.kerberos.krb5.file to be set in your nifi.properties</description><displayName>Kerberos Principal</displayName><dynamic>false</dynamic><name>Kerberos Principal</name><required>false</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry><entry><key>Kerberos Keytab</key><value><description>Kerberos keytab associated with the principal. Requires nifi.kerberos.krb5.file to be set in your nifi.properties</description><displayName>Kerberos Keytab</displayName><dynamic>false</dynamic><name>Kerberos Keytab</name><required>false</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry><entry><key>Kerberos Relogin Period</key><value><defaultValue>4 hours</defaultValue><description>Period of time which should pass before attempting a kerberos relogin</description><displayName>Kerberos Relogin Period</displayName><dynamic>false</dynamic><name>Kerberos Relogin Period</name><required>false</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry><entry><key>Directory</key><value><description>The parent HDFS directory to which files should be written</description><displayName>Directory</displayName><dynamic>false</dynamic><name>Directory</name><required>true</required><sensitive>false</sensitive><supportsEl>true</supportsEl></value></entry><entry><key>Conflict Resolution Strategy</key><value><allowableValues><displayName>replace</displayName><value>replace</value></allowableValues><allowableValues><displayName>ignore</displayName><value>ignore</value></allowableValues><allowableValues><displayName>fail</displayName><value>fail</value></allowableValues><defaultValue>fail</defaultValue><description>Indicates what should happen when a file with the same name already exists in the output directory</description><displayName>Conflict Resolution Strategy</displayName><dynamic>false</dynamic><name>Conflict Resolution Strategy</name><required>true</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry><entry><key>Block Size</key><value><description>Size of each block as written to HDFS. This overrides the Hadoop Configuration</description><displayName>Block Size</displayName><dynamic>false</dynamic><name>Block Size</name><required>false</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry><entry><key>IO Buffer Size</key><value><description>Amount of memory to use to buffer file contents during IO. This overrides the Hadoop Configuration</description><displayName>IO Buffer Size</displayName><dynamic>false</dynamic><name>IO Buffer Size</name><required>false</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry><entry><key>Replication</key><value><description>Number of times that HDFS will replicate each file. This overrides the Hadoop Configuration</description><displayName>Replication</displayName><dynamic>false</dynamic><name>Replication</name><required>false</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry><entry><key>Permissions umask</key><value><description>A umask represented as an octal number which determines the permissions of files written to HDFS. This overrides the Hadoop Configuration dfs.umaskmode</description><displayName>Permissions umask</displayName><dynamic>false</dynamic><name>Permissions umask</name><required>false</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry><entry><key>Remote Owner</key><value><description>Changes the owner of the HDFS file to this value after it is written. This only works if NiFi is running as a user that has HDFS super user privilege to change owner</description><displayName>Remote Owner</displayName><dynamic>false</dynamic><name>Remote Owner</name><required>false</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry><entry><key>Remote Group</key><value><description>Changes the group of the HDFS file to this value after it is written. This only works if NiFi is running as a user that has HDFS super user privilege to change group</description><displayName>Remote Group</displayName><dynamic>false</dynamic><name>Remote Group</name><required>false</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry><entry><key>Compression codec</key><value><allowableValues><displayName>NONE</displayName><value>NONE</value></allowableValues><allowableValues><displayName>DEFAULT</displayName><value>DEFAULT</value></allowableValues><allowableValues><displayName>BZIP</displayName><value>BZIP</value></allowableValues><allowableValues><displayName>GZIP</displayName><value>GZIP</value></allowableValues><allowableValues><displayName>LZ4</displayName><value>LZ4</value></allowableValues><allowableValues><displayName>SNAPPY</displayName><value>SNAPPY</value></allowableValues><allowableValues><displayName>AUTOMATIC</displayName><value>AUTOMATIC</value></allowableValues><defaultValue>NONE</defaultValue><description></description><displayName>Compression codec</displayName><dynamic>false</dynamic><name>Compression codec</name><required>true</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry></descriptors><lossTolerant>false</lossTolerant><penaltyDuration>30 sec</penaltyDuration><properties><entry><key>Hadoop Configuration Resources</key><value>/etc/hadoop/conf.cloudera.hdfs/core-site.xml,/etc/hadoop/conf.cloudera.hdfs/hdfs-site.xml</value></entry><entry><key>Kerberos Principal</key></entry><entry><key>Kerberos Keytab</key></entry><entry><key>Kerberos Relogin Period</key><value>4 hours</value></entry><entry><key>Directory</key><value>/user/cloudera/nifi-output/${now():format('yyyy-MM')}/${now():format('dd')}/${now():format('HH')}</value></entry><entry><key>Conflict Resolution Strategy</key><value>fail</value></entry><entry><key>Block Size</key></entry><entry><key>IO Buffer Size</key></entry><entry><key>Replication</key></entry><entry><key>Permissions umask</key></entry><entry><key>Remote Owner</key></entry><entry><key>Remote Group</key></entry><entry><key>Compression codec</key><value>NONE</value></entry></properties><runDurationMillis>0</runDurationMillis><schedulingPeriod>0 sec</schedulingPeriod><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><yieldDuration>1 sec</yieldDuration></config><name>PutHDFS</name><relationships><autoTerminate>true</autoTerminate><description>Files that could not be written to HDFS for some reason are transferred to this relationship</description><name>failure</name></relationships><relationships><autoTerminate>true</autoTerminate><description>Files that have been successfully written to HDFS are transferred to this relationship</description><name>success</name></relationships><state>STOPPED</state><style/><supportsEventDriven>false</supportsEventDriven><supportsParallelProcessing>true</supportsParallelProcessing><type>org.apache.nifi.processors.hadoop.PutHDFS</type></processors></snippet><timestamp>07/04/2016 07:38:39 PDT</timestamp></template>